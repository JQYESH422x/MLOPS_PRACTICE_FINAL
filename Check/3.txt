DL 3

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Embedding, GlobalAveragePooling1D
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load and preprocess data
max_words=10000
max_len=200
(X_train,y_train),(X_test,y_test)=imdb.load_data(num_words=max_words)
print(len(X_train[0]))

# Padding to Ensure Uniform Input Size
X_train=pad_sequences(X_train,maxlen=max_len)
X_test=pad_sequences(X_test,maxlen=max_len)
print(len(X_train[0]))
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

# Build the DNN
model=Sequential([
    Embedding(input_dim=max_words,output_dim=128,input_length=max_len),
    GlobalAveragePooling1D(),
    Dense(64,activation="relu"),
    Dense(1,activation="sigmoid")
])

# Compile the model
model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])

# Train the model
history=model.fit(X_train,y_train,epochs=5,batch_size=64,validation_data=(X_test,y_test))

# Evaluation
loss,accuracy=model.evaluate(X_test,y_test)
print("Loss :",loss)
print("Accuracy :",accuracy*100)

# Plotting
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
